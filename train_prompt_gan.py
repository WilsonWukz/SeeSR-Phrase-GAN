import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from Prompt_Dataset import PromptDataset
from prompt_gan import PromptGenerator, PromptDiscriminator
import clip
import pickle
from tqdm import tqdm


def train_prompt_gan(imgid_to_vector, vocab_words, image_dir):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # CLIP for text alignment only
    clip_model, preprocess = clip.load("ViT-B/32", device=device)

    # annotation_csv = "E:/Sydney_study/5329/A2/flickr/flickr_annotations_30k.csv"

    dataset = PromptDataset(
        image_dir=image_dir,
        imgid_to_vector=imgid_to_vector,
        vocab_words=vocab_words,
        imageid_to_filename=imageid_to_filename
    )
    loader = DataLoader(dataset, batch_size=64, shuffle=True)

    output_dim = len(vocab_words)

    G = PromptGenerator(output_dim=output_dim).to(device)   # ç”¨ U-Net æ›¿ä»£ MLP
    D = PromptDiscriminator(input_dim=output_dim).to(device)
    print(output_dim)
    print("Generator:", G)
    print("Discriminator:", D)

    opt_G = torch.optim.Adam(G.parameters(), lr=5e-4)
    opt_D = torch.optim.Adam(D.parameters(), lr=1e-4)

    for epoch in tqdm(range(25)):
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)

            # ========== Train D ==========
            with torch.no_grad():
                fake = torch.sigmoid(G(imgs)).detach()  # ğŸ” è¾“å…¥å›¾åƒï¼ˆä¸æ˜¯ CLIP ç‰¹å¾ï¼‰

            real_score = D(labels)
            fake_score = D(fake)
            loss_D = -torch.mean(torch.log(real_score + 1e-5) + torch.log(1 - fake_score + 1e-5))
            opt_D.zero_grad()
            loss_D.backward()
            opt_D.step()

            # ========== Train G ==========
            pred = torch.sigmoid(G(imgs))  # ğŸ” è¾“å…¥å›¾åƒï¼ˆä¸æ˜¯ CLIP ç‰¹å¾ï¼‰
            fake_score = D(pred)
            loss_G_adv = -torch.mean(torch.log(fake_score + 1e-5))

            norm_pred = F.normalize(pred, dim=1)  # å¯¹æ¯ä¸ªå‘é‡è¿›è¡Œå½’ä¸€åŒ– [B, output_dim]
            sim_matrix = torch.matmul(norm_pred, norm_pred.T)  # [B, B]ï¼Œæ¯å¯¹æ ·æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
            batch_size = pred.size(0)
            mask = torch.eye(batch_size, device=pred.device).bool()
            sim_matrix.masked_fill_(mask, 0)  # ä¸è€ƒè™‘å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±ï¼‰
            diversity_loss = sim_matrix.mean()  # è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºè¶Šâ€œå¤šæ ·â€

            # åŠ æƒåŠ å…¥åˆ° Generator æ€» loss
            Î»_div = 0.1  # æ§åˆ¶å¤šæ ·æ€§æ­£åˆ™é¡¹æƒé‡

            # CLIP similarity loss
            captions = []
            for p in pred.cpu():
                indices = (p > 0.9).nonzero().flatten().tolist()
                if len(indices) > 10:
                    indices = indices[:10]  # é˜²æ­¢è¿‡é•¿
                caption = " ".join([vocab_words[i] for i in indices])
                if len(caption.strip()) == 0:
                    caption = "realistic"
                captions.append(caption)

            text_inputs = clip.tokenize(captions).to(device)
            with torch.no_grad():
                text_feat = clip_model.encode_text(text_inputs)
                img_feat = clip_model.encode_image(imgs).float()  # ğŸ” ç”¨äºè®¡ç®—è¯­ä¹‰å¯¹é½ loss

            sim = F.cosine_similarity(img_feat, text_feat).mean()
            loss_clip = 1 - sim

            loss_G = loss_G_adv + 0.5 * loss_clip + Î»_div * diversity_loss
            opt_G.zero_grad()
            loss_G.backward()
            opt_G.step()
            # print(captions)
        print(f"Epoch {epoch}: D={loss_D.item():.4f}, G={loss_G.item():.4f}, Div={diversity_loss.item():.4f}")

    torch.save(G.state_dict(), "E:/Sydney_study/5329/A2/flickr/model/coco_generator2.pth")


# ====== æ•°æ®åŠ è½½å…¥å£ç‚¹ ======
with open("E:/Sydney_study/5329/A2/COCO/imgid_to_vector.pkl", "rb") as f:
    imgid_to_vector = pickle.load(f)

with open("E:/Sydney_study/5329/A2/COCO/vocab_words.txt", "r", encoding="utf-8") as f:
    vocab_words = [line.strip() for line in f.readlines()]

with open("E:/Sydney_study/5329/A2/COCO/image_id_to_filename.pkl", "rb") as f:
    imageid_to_filename = pickle.load(f)

train_prompt_gan(imgid_to_vector, vocab_words, "E:/Sydney_study/5329/A2/COCO/data/val2014/")
