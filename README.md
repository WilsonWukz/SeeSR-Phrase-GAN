With the rapid development of text-image fusion models and the emergence of models with degradation-aware capabilities, some image reconstruction tasks involving severely degraded clarity have been significantly improved. However, when facing problems of complex image information, the generated image labels may lack details, textures, and label state information, leading to potential deviations in the reconstruction model. 

To further support the applicability of image enhancement in small-scale scenarios, we propose a GAN-based method to enrich the textual semantics extracted from the images to be reconstructed. This is achieved by providing adjective-level keywords and fusing them with the hard prompts generated by the degradation-aware prompt extractor, in order to compensate for additional information that is often neglected in the hard prompts.
This project is a demonstration of the research track on deep learning courses at the University of Sydney. It provides all the pre-trained model weights required to improve Seesr, and the relevant code is available on GitHub
